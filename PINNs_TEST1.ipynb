{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Epoch 10/2000 [00:01:00], Loss: 1.706640, PDE_x: 0.004439, PDE_y: 0.716667, BC: 0.264428, LR: 0.001000\n",
      "Epoch 20/2000 [00:01:41], Loss: 1.289925, PDE_x: 0.007161, PDE_y: 0.543887, BC: 0.187830, LR: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 424\u001b[0m\n\u001b[0;32m    421\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)  \u001b[38;5;66;03m# ใช้ parameters จาก network เดียว\u001b[39;00m\n\u001b[0;32m    423\u001b[0m fem_data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFEM1_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 424\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfem_data_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfem_data_file\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# เรียกใช้แบบระบุชื่อ parameter\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[0;32m    427\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 275\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, optimizer, n_epochs, fem_data_file)\u001b[0m\n\u001b[0;32m    272\u001b[0m     multi_view\u001b[38;5;241m.\u001b[39mtransfer_knowledge(prev_net, net, layer_mapping)\n\u001b[0;32m    274\u001b[0m loss_pde_x, loss_pde_y \u001b[38;5;241m=\u001b[39m PDE(x, y, net)\n\u001b[1;32m--> 275\u001b[0m loss_bc \u001b[38;5;241m=\u001b[39m \u001b[43mBC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mloss_pde_x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m loss_pde_y \u001b[38;5;241m+\u001b[39m loss_bc\n\u001b[0;32m    279\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[5], line 113\u001b[0m, in \u001b[0;36mBC\u001b[1;34m(xy, net)\u001b[0m\n\u001b[0;32m    109\u001b[0m bc_slope \u001b[38;5;241m=\u001b[39m BC_Slope(x, y)\n\u001b[0;32m    111\u001b[0m huber \u001b[38;5;241m=\u001b[39m HuberLoss(delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m loss \u001b[38;5;241m=\u001b[39m huber(u[bc_b], \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbc_b\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m huber(v[bc_b], torch\u001b[38;5;241m.\u001b[39mzeros_like(v[bc_b]))\n\u001b[0;32m    114\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m huber(u[bc_l], torch\u001b[38;5;241m.\u001b[39mzeros_like(u[bc_l]))\n\u001b[0;32m    115\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m huber(u[bc_r], torch\u001b[38;5;241m.\u001b[39mzeros_like(u[bc_r]))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.path import Path\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "import copy\n",
    "import time \n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set default tensor type to float32\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = nn.Linear(2, 32)\n",
    "        self.hidden2 = nn.Linear(32, 32)\n",
    "        self.hidden3 = nn.Linear(32, 32)\n",
    "        self.output = nn.Linear(32, 2)\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = torch.tanh(self.hidden3(x))\n",
    "        return self.output(x)  # return [u, v]\n",
    "    # Define the problem domain using the given vertices\n",
    "vertices = np.array([\n",
    "    [0, 0],\n",
    "    [0, 5],\n",
    "    [5, 5],\n",
    "    [8, 2],\n",
    "    [10, 2],\n",
    "    [10,0],\n",
    "    [0,0]  # Closing the polygon\n",
    "],dtype=np.float32)\n",
    "path = Path(vertices)\n",
    "def in_domain(x, y):\n",
    "    points = np.column_stack((x.cpu().numpy(), y.cpu().numpy()))\n",
    "    return torch.tensor(path.contains_points(points), dtype=torch.bool, device=device)\n",
    "\n",
    "# Define boundary conditions\n",
    "def BC_bottom(x, y):\n",
    "    return ((y == 0) & (x >= 0) & (x <= 10)).squeeze()\n",
    "\n",
    "def BC_left(x, y):\n",
    "    return ((x == 0) & (y >= 0) & (y <= 5)).squeeze()\n",
    "\n",
    "def BC_top(x, y):\n",
    "    tol = 1e-6\n",
    "    return (((y == 5) & (x >= 0) & (x <= 5)) | \n",
    "            (abs((y - 10) + x) < tol) & (x > 5) & (x < 8) |\n",
    "            (y == 2) & (x >= 8) & (x <= 10)).squeeze()\n",
    "\n",
    "def BC_right(x, y):\n",
    "    return ((x == 10) & (y >= 0) & (y <= 2)).squeeze()\n",
    "\n",
    "def BC_Slope(x, y):\n",
    "    tol = 1e-6\n",
    "    return (abs((y - 10) + x) < tol) & (x > 5) & (x < 8).squeeze()\n",
    "\n",
    "\n",
    "class HuberLoss:\n",
    "    def __init__(self, delta=0.5):    #<------------------------ Delta Huber loss\n",
    "        self.delta = delta\n",
    "        \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        error = torch.abs(y_true - y_pred)\n",
    "        quadratic = torch.min(error, torch.tensor(self.delta))\n",
    "        linear = error - quadratic\n",
    "        return torch.mean(0.5 * quadratic ** 2 + self.delta * linear)\n",
    "\n",
    "class MultiViewPINN:\n",
    "    def __init__(self):\n",
    "        # Keep track of learned knowledge from different views\n",
    "        self.view_knowledge = {}\n",
    "        self.huber = HuberLoss(delta=0.5)   #<------------------------ Delta Huber loss\n",
    "        \n",
    "    def transfer_knowledge(self, source_net, target_net, layer_mapping):\n",
    "        \"\"\"Transfer weights from source network to target network\"\"\"\n",
    "        for source_layer, target_layer in layer_mapping:\n",
    "            target_net.state_dict()[target_layer].data.copy_(\n",
    "                source_net.state_dict()[source_layer].data\n",
    "            )\n",
    "\n",
    "def BC(xy, net):\n",
    "    x, y = xy[:, 0].unsqueeze(1), xy[:, 1].unsqueeze(1)\n",
    "    \n",
    "    output = net(xy)\n",
    "    u = output[:, 0:1]  # แยก output channel แรกเป็น u\n",
    "    v = output[:, 1:2]  # แยก output channel ที่สองเป็น v\n",
    "    \n",
    "    bc_b = BC_bottom(x, y)\n",
    "    bc_l = BC_left(x, y)\n",
    "    bc_t = BC_top(x, y)\n",
    "    bc_r = BC_right(x, y)\n",
    "    bc_slope = BC_Slope(x, y)\n",
    "    \n",
    "    huber = HuberLoss(delta=0.5)   #<------------------------ Delta Huber loss\n",
    "    \n",
    "    loss = huber(u[bc_b], torch.zeros_like(u[bc_b])) + huber(v[bc_b], torch.zeros_like(v[bc_b]))\n",
    "    loss += huber(u[bc_l], torch.zeros_like(u[bc_l]))\n",
    "    loss += huber(u[bc_r], torch.zeros_like(u[bc_r]))\n",
    "    \n",
    "    xy_top = xy[bc_t].requires_grad_(True)\n",
    "    output_top = net(xy_top)\n",
    "    u_top = output_top[:, 0:1]\n",
    "    v_top = output_top[:, 1:2]\n",
    "    \n",
    "    u_x_top = torch.autograd.grad(u_top.sum(), xy_top, create_graph=True)[0][:, 0]\n",
    "    u_y_top = torch.autograd.grad(u_top.sum(), xy_top, create_graph=True)[0][:, 1]\n",
    "    v_x_top = torch.autograd.grad(v_top.sum(), xy_top, create_graph=True)[0][:, 0]\n",
    "    v_y_top = torch.autograd.grad(v_top.sum(), xy_top, create_graph=True)[0][:, 1]\n",
    "    \n",
    "    E = 5.0  \n",
    "    nu = 0.3 \n",
    "    \n",
    "    sigma_xx_top = (E / ((1 + nu) * (1 - 2 * nu))) * (u_x_top + nu * v_y_top)\n",
    "    sigma_yy_top = (E / ((1 + nu) * (1 - 2 * nu))) * (v_y_top + nu * u_x_top)\n",
    "    sigma_xy_top = E / (2 * (1 + nu)) * (u_y_top + v_x_top)\n",
    "    \n",
    "    loss += huber(sigma_xy_top, torch.zeros_like(sigma_xy_top))\n",
    "    loss += huber(sigma_xx_top, torch.zeros_like(sigma_xx_top))\n",
    "    loss += huber(sigma_yy_top, torch.zeros_like(sigma_yy_top))\n",
    "    \n",
    "    xy_right = xy[bc_r].requires_grad_(True)\n",
    "    output_right = net(xy_right)\n",
    "    u_right = output_right[:, 0:1]\n",
    "    v_right = output_right[:, 1:2]\n",
    "    \n",
    "    u_y_right = torch.autograd.grad(u_right.sum(), xy_right, create_graph=True)[0][:, 1]\n",
    "    v_x_right = torch.autograd.grad(v_right.sum(), xy_right, create_graph=True)[0][:, 0]\n",
    "    \n",
    "    sigma_xy_right = E / (2 * (1 + nu)) * (u_y_right + v_x_right)\n",
    "    loss += huber(sigma_xy_right, torch.zeros_like(sigma_xy_right))\n",
    "\n",
    "    xy_left = xy[bc_l].requires_grad_(True)\n",
    "    output_left = net(xy_left)\n",
    "    u_left = output_left[:, 0:1]\n",
    "    v_left = output_left[:, 1:2]\n",
    "    \n",
    "    u_y_left = torch.autograd.grad(u_left.sum(), xy_left, create_graph=True)[0][:, 1]\n",
    "    v_x_left = torch.autograd.grad(v_left.sum(), xy_left, create_graph=True)[0][:, 0]\n",
    "    \n",
    "    sigma_xy_left = E / (2 * (1 + nu)) * (u_y_left + v_x_left)\n",
    "    loss += huber(sigma_xy_left, torch.zeros_like(sigma_xy_left))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def load_fem_data(filename='FEM1_data.csv'):\n",
    "    df = pd.read_csv(filename)\n",
    "    x = torch.tensor(df['X'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y = torch.tensor(df['Y'].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def generate_training_data(fem_data_file, n_boundary=2000):\n",
    "    x, y = load_fem_data(fem_data_file)\n",
    "    \n",
    "    # Keep only points inside the domain\n",
    "    mask = in_domain(x, y)\n",
    "    x, y = x[mask], y[mask]\n",
    "    \n",
    "    # Generate boundary points\n",
    "    t = torch.linspace(0, 1, n_boundary, device=device).unsqueeze(1)\n",
    "    \n",
    "    # Define the boundary segments\n",
    "    segments = [\n",
    "        ([0, 0, 0], [0, 5, 5]),  # Left boundary\n",
    "        ([0, 5, 8, 10, 10], [5, 5, 2, 2, 0]),  # Top and right boundary\n",
    "        ([10, 0], [0, 0])  # Bottom boundary\n",
    "    ]\n",
    "    \n",
    "    x_b = []\n",
    "    y_b = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        x_seg = torch.tensor(np.interp(t.cpu().numpy(), np.linspace(0, 1, len(segment[0])), segment[0]), dtype=torch.float32, device=device)\n",
    "        y_seg = torch.tensor(np.interp(t.cpu().numpy(), np.linspace(0, 1, len(segment[1])), segment[1]), dtype=torch.float32, device=device)\n",
    "        x_b.append(x_seg)\n",
    "        y_b.append(y_seg)\n",
    "    \n",
    "    x_b = torch.cat(x_b)\n",
    "    y_b = torch.cat(y_b)\n",
    "    \n",
    "    return x, y, x_b, y_b\n",
    "\n",
    "\n",
    "def PDE(x, y, net):\n",
    "    xy = torch.cat([x, y], dim=1)\n",
    "    xy.requires_grad = True\n",
    "    \n",
    "    output = net(xy)\n",
    "    u = output[:, 0:1]\n",
    "    v = output[:, 1:2]\n",
    "    \n",
    "    u_x = torch.autograd.grad(u.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    u_y = torch.autograd.grad(u.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    v_x = torch.autograd.grad(v.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    v_y = torch.autograd.grad(v.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    \n",
    "    E = 5\n",
    "    nu = 0.3\n",
    "    gamma = 1.8\n",
    "    \n",
    "    sigma_xx = E / (1 - nu**2) * (u_x + nu * v_y)\n",
    "    sigma_yy = E / (1 - nu**2) * (v_y + nu * u_x)\n",
    "    sigma_xy = E / (2 * (1 + nu)) * (u_y + v_x)\n",
    "    \n",
    "    f_x = torch.zeros_like(x)\n",
    "    f_y = -gamma * torch.ones_like(y)\n",
    "    \n",
    "    R_x = torch.autograd.grad(sigma_xx.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1) + \\\n",
    "          torch.autograd.grad(sigma_xy.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1) + f_x\n",
    "    R_y = torch.autograd.grad(sigma_xy.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1) + \\\n",
    "          torch.autograd.grad(sigma_yy.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1) + f_y\n",
    "    \n",
    "    huber = HuberLoss(delta=0.5)  #<------------------------ Delta Huber loss\n",
    "    loss_x = huber(R_x, torch.zeros_like(R_x))\n",
    "    loss_y = huber(R_y, torch.zeros_like(R_y))\n",
    "    \n",
    "    return loss_x, loss_y\n",
    "\n",
    "def calculate_batch_flops(batch_size):\n",
    "    # Network parameters\n",
    "    input_size = 2  # x, y coordinates\n",
    "    hidden_size = 32 \n",
    "    num_layers = 3\n",
    "    output_size = 2  # u, v displacements\n",
    "    \n",
    "    # Forward pass operations\n",
    "    linear_ops = batch_size * (\n",
    "        (input_size * hidden_size) +  \n",
    "        (hidden_size * hidden_size * (num_layers-1)) +  \n",
    "        (hidden_size * output_size)  \n",
    "    )\n",
    "    activation_ops = batch_size * hidden_size * num_layers\n",
    "    \n",
    "    # PDE calculations\n",
    "    derivative_ops = batch_size * (\n",
    "        4 * (input_size * hidden_size + hidden_size * hidden_size * (num_layers-1) + hidden_size * output_size) +\n",
    "        12 * hidden_size * num_layers\n",
    "    )\n",
    "    \n",
    "    stress_ops = batch_size * (3 * 4)\n",
    "    bc_ops = batch_size * (\n",
    "        2 * (input_size * hidden_size + hidden_size * hidden_size * (num_layers-1) + hidden_size * output_size) +\n",
    "        6 * hidden_size * num_layers\n",
    "    )\n",
    "    \n",
    "    total_ops = linear_ops + activation_ops + derivative_ops + stress_ops + bc_ops\n",
    "    return total_ops\n",
    "\n",
    "def calculate_tflops(start_time, batch_flops):\n",
    "    \"\"\"คำนวณ TFLOPS โดยใช้เวลาที่ผ่านไปและจำนวน FLOPs ในแต่ละ batch\"\"\"\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    if elapsed <= 0:  # ป้องกันการหารด้วย 0\n",
    "        return 0\n",
    "    tflops = (batch_flops / elapsed) / 1e12\n",
    "    return tflops  # คืนค่าจริงโดยไม่มีการ clamp ขั้นต่ำ\n",
    "\n",
    "\n",
    "def train(net, optimizer, n_epochs, fem_data_file):\n",
    "    def calculate_batch_flops(batch_size):\n",
    "        # Network parameters\n",
    "        input_size = 2  # x, y coordinates\n",
    "        hidden_size = 32 \n",
    "        num_layers = 3\n",
    "        output_size = 2  # u, v displacements\n",
    "        \n",
    "        # Forward pass operations\n",
    "        linear_ops = batch_size * (\n",
    "            (input_size * hidden_size) +  # First layer\n",
    "            (hidden_size * hidden_size * (num_layers-1)) +  # Hidden layers\n",
    "            (hidden_size * output_size)  # Output layer\n",
    "        )\n",
    "        activation_ops = batch_size * hidden_size * num_layers  # tanh operations\n",
    "        \n",
    "        # PDE calculations (each derivative requires a backward pass)\n",
    "        derivative_ops = batch_size * (\n",
    "            4 * (input_size * hidden_size + hidden_size * hidden_size * (num_layers-1) + hidden_size * output_size) + # First derivatives (u_x, u_y, v_x, v_y)\n",
    "            12 * hidden_size * num_layers  # Activation functions in backward pass\n",
    "        )\n",
    "        \n",
    "        # Stress calculations\n",
    "        stress_ops = batch_size * (\n",
    "            3 * 4  # Basic arithmetic for each stress component\n",
    "        )\n",
    "        \n",
    "        # BC operations\n",
    "        bc_ops = batch_size * (\n",
    "            2 * (input_size * hidden_size + hidden_size * hidden_size * (num_layers-1) + hidden_size * output_size) + # Forward pass for BC\n",
    "            6 * hidden_size * num_layers  # Activation functions\n",
    "        )\n",
    "        \n",
    "        total_ops = linear_ops + activation_ops + derivative_ops + stress_ops + bc_ops\n",
    "        return total_ops\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_flops = time.perf_counter()\n",
    "    total_flops = 0\n",
    "\n",
    "    multi_view = MultiViewPINN()\n",
    "    huber = HuberLoss(delta=0.5) #<------------------------ Delta Huber loss\n",
    "    \n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=1e-6)\n",
    "# Removed: best_loss, patience, max_patience, prev_net variables\n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    max_patience = 3000\n",
    "    prev_net = None\n",
    "    \n",
    "    # สร้าง lists สำหรับเก็บข้อมูลระหว่างการเทรน\n",
    "    log_data = {\n",
    "        'epoch': [],\n",
    "        'total_loss': [],\n",
    "        'pde_x_loss': [],\n",
    "        'pde_y_loss': [],\n",
    "        'bc_loss': [],\n",
    "        'learning_rate': [],\n",
    "        'elapsed_time': [],\n",
    "        'timestamp': [],\n",
    "        'tflops': []\n",
    "    }\n",
    "    \n",
    "    # สร้าง lists สำหรับเก็บข้อมูลสำหรับพล็อตกราฟ\n",
    "    epoch_history = []\n",
    "    loss_history = []\n",
    "    pde_x_history = []\n",
    "    pde_y_history = []\n",
    "    bc_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y, x_b, y_b = generate_training_data(fem_data_file, 30000)\n",
    "        batch_size = len(x) + len(x_b)  # Total points for this batch\n",
    "        \n",
    "        # Calculate FLOPs for this batch\n",
    "        batch_flops = calculate_batch_flops(batch_size)\n",
    "        total_flops += batch_flops\n",
    "        \n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "        xy_b = torch.cat([x_b, y_b], dim=1)\n",
    "        \n",
    "        loss_pde_x, loss_pde_y = PDE(x, y, net)\n",
    "        loss_bc = BC(xy_b, net)\n",
    "        loss = 2*loss_pde_x + 2*loss_pde_y + loss_bc\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        current_tflops = calculate_tflops(start_flops, batch_flops)\n",
    "        \n",
    "        # บันทึกข้อมูลทุก 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            log_data['epoch'].append(epoch)\n",
    "            log_data['total_loss'].append(loss.item())\n",
    "            log_data['pde_x_loss'].append(loss_pde_x.item())\n",
    "            log_data['pde_y_loss'].append(loss_pde_y.item())\n",
    "            log_data['bc_loss'].append(loss_bc.item())\n",
    "            log_data['learning_rate'].append(scheduler.get_last_lr()[0])\n",
    "            log_data['elapsed_time'].append(elapsed_time)\n",
    "            log_data['timestamp'].append(time.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            log_data['tflops'].append(current_tflops)\n",
    "            \n",
    "            # เก็บข้อมูลสำหรับพล็อตกราฟ\n",
    "            epoch_history.append(epoch)\n",
    "            loss_history.append(loss.item())\n",
    "            pde_x_history.append(loss_pde_x.item())\n",
    "            pde_y_history.append(loss_pde_y.item())\n",
    "            bc_history.append(loss_bc.item())\n",
    "            \n",
    "            # แสดงผลและบันทึก CSV\n",
    "            hours = int(elapsed_time // 3600)\n",
    "            minutes = int((elapsed_time % 3600) // 60)\n",
    "            seconds = int(elapsed_time % 60)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{n_epochs} [{hours:02d}:{minutes:02d}:{seconds:02d}], '\n",
    "                  f'Loss: {loss.item():.6f}, '\n",
    "                  f'PDE_x: {loss_pde_x.item():.6f}, PDE_y: {loss_pde_y.item():.6f}, '\n",
    "                  f'BC: {loss_bc.item():.6f}, LR: {scheduler.get_last_lr()[0]:.6f}, '\n",
    "                  f'TFLOPs/s: {current_tflops:.6f}')\n",
    "            \n",
    "            # บันทึก CSV\n",
    "            df_log = pd.DataFrame(log_data)\n",
    "            df_log.to_csv('training_log.csv', index=False)\n",
    "\n",
    "    # คำนวณข้อมูลสรุปเมื่อเสร็จสิ้นการเทรน\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    average_tflops = total_flops / (1e12 * training_time)\n",
    "    \n",
    "    hours = int(training_time // 3600)\n",
    "    minutes = int((training_time % 3600) // 60)\n",
    "    seconds = int(training_time % 60)\n",
    "    \n",
    "    # แสดงผลสรุป\n",
    "    print(f\"\\nTraining completed in {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "    print(f\"Average computational throughput: {average_tflops:.2f} TFLOPs/s\")\n",
    "    \n",
    "    # บันทึกสรุปการ training\n",
    "    with open('training_summary.txt', 'w') as f:\n",
    "        f.write(f\"Training Summary\\n\")\n",
    "        f.write(f\"===============\\n\")\n",
    "        f.write(f\"Total training time: {hours:02d}:{minutes:02d}:{seconds:02d}\\n\")\n",
    "        f.write(f\"Total epochs: {n_epochs}\\n\")\n",
    "        f.write(f\"Best loss: {best_loss:.6f}\\n\")\n",
    "        f.write(f\"Final loss: {loss.item():.6f}\\n\")\n",
    "        f.write(f\"Final PDE_x loss: {loss_pde_x.item():.6f}\\n\")\n",
    "        f.write(f\"Final PDE_y loss: {loss_pde_y.item():.6f}\\n\")\n",
    "        f.write(f\"Final BC loss: {loss_bc.item():.6f}\\n\")\n",
    "        f.write(f\"Final learning rate: {scheduler.get_last_lr()[0]:.6f}\\n\")\n",
    "        f.write(f\"Average computational throughput: {average_tflops:.5f} TFLOPs/s\\n\")\n",
    "        f.write(f\"Total FLOPs: {total_flops:,}\\n\")\n",
    "    \n",
    "    # พล็อตกราฟ Loss history\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot total loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epoch_history, loss_history, 'b-', label='Total Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Training Loss History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot component losses\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epoch_history, pde_x_history, 'r-', label='PDE_x Loss')\n",
    "    plt.plot(epoch_history, pde_y_history, 'g-', label='PDE_y Loss')\n",
    "    plt.plot(epoch_history, bc_history, 'b-', label='BC Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Component Losses (log scale)')\n",
    "    plt.title('Component Losses History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # บันทึก CSV ครั้งสุดท้าย\n",
    "    df_log = pd.DataFrame(log_data)\n",
    "    df_log.to_csv('training_log.csv', index=False)\n",
    "    \n",
    "    return net, log_data\n",
    "    \n",
    "def plot_results(net, fem_data_file='FEM1_data.csv', output_filename='PiNN1_data.csv'):\n",
    "    # Load FEM data\n",
    "    df_fem = pd.read_csv(fem_data_file)\n",
    "    X = df_fem['X'].values\n",
    "    Y = df_fem['Y'].values\n",
    "    \n",
    "    # Create tensor from FEM data\n",
    "    XY = torch.tensor(np.column_stack([X, Y]), dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Compute displacements and stresses for all points\n",
    "    XY.requires_grad_(True)\n",
    "    with torch.enable_grad():\n",
    "        output = net(XY)\n",
    "        U = output[:, 0:1]  # แยก u\n",
    "        V = output[:, 1:2]  # แยก v\n",
    "        \n",
    "        U_x = torch.autograd.grad(U.sum(), XY, create_graph=True)[0][:, 0]\n",
    "        U_y = torch.autograd.grad(U.sum(), XY, create_graph=True)[0][:, 1]\n",
    "        V_x = torch.autograd.grad(V.sum(), XY, create_graph=True)[0][:, 0]\n",
    "        V_y = torch.autograd.grad(V.sum(), XY, create_graph=True)[0][:, 1]\n",
    "        \n",
    "        E = 5  # Young's modulus\n",
    "        nu = 0.3  # Poisson's ratio\n",
    "        \n",
    "        sigma_xx = E / (1 - nu**2) * (U_x + nu * V_y)\n",
    "        sigma_yy = E / (1 - nu**2) * (V_y + nu * U_x)\n",
    "        sigma_xy = E / (2 * (1 + nu)) * (U_y + V_x)\n",
    "    \n",
    "    # Move tensors to CPU and convert to numpy\n",
    "    U_full = U.detach().cpu().numpy().squeeze()/1000\n",
    "    V_full = V.detach().cpu().numpy().squeeze()/1000\n",
    "    sigma_xx_full = sigma_xx.detach().cpu().numpy().squeeze()*10\n",
    "    sigma_yy_full = sigma_yy.detach().cpu().numpy().squeeze()*10\n",
    "    sigma_xy_full = sigma_xy.detach().cpu().numpy().squeeze()*10\n",
    "    \n",
    "    # Calculate displacement magnitude\n",
    "    magnitude = np.sqrt(U_full**2 + V_full**2)\n",
    "    \n",
    "    # Create DataFrame for CSV export\n",
    "    df_out = pd.DataFrame({\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'ux': U_full,\n",
    "        'uy': V_full,\n",
    "        'sigma_xx': sigma_xx_full,\n",
    "        'sigma_yy': sigma_yy_full,\n",
    "        'sigma_xy': sigma_xy_full,\n",
    "        'magnitude': magnitude\n",
    "    })\n",
    "\n",
    "    # Save the output to a CSV file\n",
    "    df_out.to_csv(output_filename, index=False)\n",
    "    print(f\"Results saved to {output_filename}\")\n",
    "    \n",
    "    # Apply domain mask for plotting\n",
    "    mask = in_domain(XY[:, 0], XY[:, 1])\n",
    "    mask_cpu = mask.cpu().numpy()\n",
    "      \n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    net = Net().to(device)  # สร้าง network เดียว\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)  # ใช้ parameters จาก network เดียว\n",
    "    \n",
    "    fem_data_file = 'FEM1_data.csv'\n",
    "    trained_net, _ = train(net=net, optimizer=optimizer, n_epochs=5000, fem_data_file=fem_data_file)\n",
    "\n",
    "    try:\n",
    "        plot_results(trained_net, fem_data_file=fem_data_file) \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plot_results: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
